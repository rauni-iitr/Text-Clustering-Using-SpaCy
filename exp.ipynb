{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sentence = 'The brown fox is quick and he is jumping over the lazy dog'\\n\\nsdp = StanfordDependencyParser()\\nresult = list(sdp.raw_parse(sentence))\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "'''import os\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from graphviz import Source\n",
    "\n",
    "# make sure nltk can find stanford-parser\n",
    "# please check your stanford-parser version from brew output (in my case 3.6.0) \n",
    "os.environ['CLASSPATH'] = r'/usr/local/Cellar/stanford-parser/3.6.0/libexec'\n",
    "\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "sdp = StanfordDependencyParser()\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
    "source.view()'''\n",
    "\n",
    "'''sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "sdp = StanfordDependencyParser()\n",
    "result = list(sdp.raw_parse(sentence))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pprint\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
    "for token in doc:\n",
    "    print(\"{2}({3}-{6}, {0}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))\n",
    "\n",
    "spacy.displacy.render(doc,options={'compact': True})\n",
    "\n",
    "x=spacy.displacy.parse_deps(doc)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_of_tuples = [('ROOT','ROOT', 'shot'),('nsubj','shot', 'I'),('det','elephant', 'an'),('dobj','shot', 'elephant'),('case','sleep', 'in'),('nmod:poss','sleep', 'my'),('nmod','shot', 'sleep')]\n",
    "\n",
    "nodes={}\n",
    "\n",
    "for i in list_of_tuples:\n",
    "    rel,parent,child=i\n",
    "    nodes[child]={'Name':child,'Relationship':rel}\n",
    "\n",
    "forest=[]\n",
    "\n",
    "for i in list_of_tuples:\n",
    "    rel,parent,child=i\n",
    "    node=nodes[child]\n",
    "\n",
    "    if parent=='ROOT':# this should be the Root Node\n",
    "            forest.append(node)\n",
    "    else:\n",
    "        parent=nodes[parent]\n",
    "        if not 'children' in parent:\n",
    "            parent['children']=[]\n",
    "        children=parent['children']\n",
    "        children.append(node)\n",
    "\n",
    "pprint.pprint (forest)\n",
    "\n",
    "pprint.pprint(x)\n",
    "\n",
    "\n",
    "print(doc)\n",
    "\n",
    "pos=[token.pos_ for token in doc]\n",
    "\n",
    "print(pos)\n",
    "\n",
    "text=nlp('''if a\n",
    "demand for the Transfer of Eligible Credit Support or Posted Credit Support is made by the\n",
    "Notification Time, then the relevant Transfer will be made not later than the close of business on the\n",
    "third Local Business Day following the date of such demand; if a demand is made after the\n",
    "Notification Time, then the relevant Transfer will be made not later than the close of business on the\n",
    "fourth Local Business Day following the date of such demand.''')\n",
    "displacy.render(text,options={'compact':True,'distance': 50\n",
    "                                          })\n",
    "\n",
    "import re\n",
    "\n",
    "list = [\"guru99 get\", \"guru99 give\", \"guru Selenium\"]\n",
    "for element in list:\n",
    "    z = re.match(\"(g\\w+)\\W(g\\w+)\", element)\n",
    "if z:\n",
    "    print((z.groups()))\n",
    "patterns = ['software testing', 'guru99']\n",
    "text = 'software testing is fun?'\n",
    "for pattern in patterns:\n",
    "    print('Looking for \"%s\" in \"%s\" ->' % (pattern, text), end=' ')\n",
    "    if re.search(pattern, text):\n",
    "        print('found a match!')\n",
    "else:\n",
    "    print('no match')\n",
    "abc = 'guru99@google.com, careerguru99@hotmail.com, users@yahoomail.com'\n",
    "emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', abc)\n",
    "for email in emails:\n",
    "    print(email)\n",
    "\n",
    "#import stanfordnlp\n",
    "#stanfordnlp.download('en')\n",
    "#nlp=stanfordnlp.Pipeline()\n",
    "\n",
    "#doc=nlp('Barack Obama was born in Hawaii.  He was elected president in 2008.')\n",
    "#doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary(docs):\n",
    "    for token in docs:\n",
    "        if token.text==';':\n",
    "            docs[token.i+1].is_sent_start=True\n",
    "        elif token.text==' ':\n",
    "            docs[token.i+1].is_sent_start=True\n",
    "    return docs\n",
    "    \n",
    "\n",
    "nlp.add_pipe(boundary,before='parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample=nlp('''Cat is an animal family; Tiger falls in the same family.\n",
    "How about if you kill some of those stupid creatures; Wont be drinking your milk i guess.''')\n",
    "for sent in sample.sents:\n",
    "    print(sent)\n",
    "\n",
    "txt=nlp('if a demand for the Transfer of Eligible Credit Support or Posted Credit Support is made by the Notification Time, then the relevant Transfer will be made not later than the close of business on the third Local Business Day following the date of such demand; if a demand is made after the Notification Time, then the relevant Transfer will be made not later than the close of business on the fourth Local Business Day following the date of such demand.')\n",
    "\n",
    "for sent in txt.sents:\n",
    "    print(sent)\n",
    "    spacy.displacy.render(sent,options={'compact': True,'distance': 100,'collapse_punct': True,'collapse_phrases':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx=nlp('''Return Amount. Subject to Paragraphs 4 and 5, upon a demand made by the Obligor on or promptly following a Valuation Date, if the Return Amount for that Valuation Date equals or exceeds the Obligee's Minimum Transfer Amount, then the Obligee will Transfer to the Obligor Posted Credit Support as specified by the Obligor in that demand having a Value as of the date of Transfer as close as practicable to the applicable Return Amount (rounded pursuant to Paragraph 13); provided, however, that where such Posted Credit Support consists of Posted Lending Collateral in the form of securities, the Obligee may Transfer to the Obligor Equivalent Collateral or repay cash equivalent thereof in the currency in which such securities are denominated or in Termination Currency. For this purpose, Transfer of the cash equivalent of any Posted Lending Collateral shall be deemed to be a return of such Posted Lending Collateral. However, solely for the purpose of this Paragraph 3(b), the Obligee's right to repay any cash equivalent is subject to the prior consent of the Obligor. Unless otherwise specified in Paragraph 13, the \"Return Amount\" applicable to the Obligee for any Valuation Date will equal the amount by which:\n",
    "\n",
    "the Value as of that Valuation Date of all Posted Credit Support held by the Obligee exceeds\n",
    "the Credit Support Amount.\n",
    "\n",
    "\"Credit Support Amount\" means, unless otherwise specified in Paragraph 13, for any Valuation Date\n",
    "(i) the Obligee's Exposure for that Valuation Date plus (ii) the aggregate of all Independent Amounts applicable to the Obligor, if any, minus (iii) all Independent Amounts applicable to the Obligee, if any, minus (iv) the Obligor's Threshold; provided however, that the Credit Support Amount will be deemed to be zero whenever the calculation of the Credit Support Amount yields a number less than zero''')\n",
    "#for i,token in enumerate(docx):\n",
    "    #print ((i,token,token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return\n",
      "Amount\n",
      ".\n",
      "Subject\n",
      "to\n",
      "Paragraphs\n",
      "4\n",
      "and\n",
      "5\n",
      ",\n",
      "upon\n",
      "a\n",
      "demand\n",
      "made\n",
      "by\n",
      "the\n",
      "Obligor\n",
      "on\n",
      "or\n",
      "promptly\n",
      "following\n",
      "a\n",
      "Valuation\n",
      "Date\n",
      ",\n",
      "if\n",
      "the\n",
      "Return\n",
      "Amount\n",
      "for\n",
      "that\n",
      "Valuation\n",
      "Date\n",
      "equals\n",
      "or\n",
      "exceeds\n",
      "the\n",
      "Obligee\n",
      "'s\n",
      "Minimum\n",
      "Transfer\n",
      "Amount\n",
      ",\n",
      "then\n",
      "the\n",
      "Obligee\n",
      "will\n",
      "Transfer\n",
      "to\n",
      "the\n",
      "Obligor\n",
      "Posted\n",
      "Credit\n",
      "Support\n",
      "as\n",
      "specified\n",
      "by\n",
      "the\n",
      "Obligor\n",
      "in\n",
      "that\n",
      "demand\n",
      "having\n",
      "a\n",
      "Value\n",
      "as\n",
      "of\n",
      "the\n",
      "date\n",
      "of\n",
      "Transfer\n",
      "as\n",
      "close\n",
      "as\n",
      "practicable\n",
      "to\n",
      "the\n",
      "applicable\n",
      "Return\n",
      "Amount\n",
      "(\n",
      "rounded\n",
      "pursuant\n",
      "to\n",
      "Paragraph\n",
      "13\n",
      ")\n",
      ";\n",
      "provided\n",
      ",\n",
      "however\n",
      ",\n",
      "that\n",
      "where\n",
      "such\n",
      "Posted\n",
      "Credit\n",
      "Support\n",
      "consists\n",
      "of\n",
      "Posted\n",
      "Lending\n",
      "Collateral\n",
      "in\n",
      "the\n",
      "form\n",
      "of\n",
      "securities\n",
      ",\n",
      "the\n",
      "Obligee\n",
      "may\n",
      "Transfer\n",
      "to\n",
      "the\n",
      "Obligor\n",
      "Equivalent\n",
      "Collateral\n",
      "or\n",
      "repay\n",
      "cash\n",
      "equivalent\n",
      "thereof\n",
      "in\n",
      "the\n",
      "currency\n",
      "in\n",
      "which\n",
      "such\n",
      "securities\n",
      "are\n",
      "denominated\n",
      "or\n",
      "in\n",
      "Termination\n",
      "Currency\n",
      ".\n",
      "For\n",
      "this\n",
      "purpose\n",
      ",\n",
      "Transfer\n",
      "of\n",
      "the\n",
      "cash\n",
      "equivalent\n",
      "of\n",
      "any\n",
      "Posted\n",
      "Lending\n",
      "Collateral\n",
      "shall\n",
      "be\n",
      "deemed\n",
      "to\n",
      "be\n",
      "a\n",
      "return\n",
      "of\n",
      "such\n",
      "Posted\n",
      "Lending\n",
      "Collateral\n",
      ".\n",
      "However\n",
      ",\n",
      "solely\n",
      "for\n",
      "the\n",
      "purpose\n",
      "of\n",
      "this\n",
      "Paragraph\n",
      "3(b\n",
      ")\n",
      ",\n",
      "the\n",
      "Obligee\n",
      "'s\n",
      "right\n",
      "to\n",
      "repay\n",
      "any\n",
      "cash\n",
      "equivalent\n",
      "is\n",
      "subject\n",
      "to\n",
      "the\n",
      "prior\n",
      "consent\n",
      "of\n",
      "the\n",
      "Obligor\n",
      ".\n",
      "Unless\n",
      "otherwise\n",
      "specified\n",
      "in\n",
      "Paragraph\n",
      "13\n",
      ",\n",
      "the\n",
      "\"\n",
      "Return\n",
      "Amount\n",
      "\"\n",
      "applicable\n",
      "to\n",
      "the\n",
      "Obligee\n",
      "for\n",
      "any\n",
      "Valuation\n",
      "Date\n",
      "will\n",
      "equal\n",
      "the\n",
      "amount\n",
      "by\n",
      "which\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "Value\n",
      "as\n",
      "of\n",
      "that\n",
      "Valuation\n",
      "Date\n",
      "of\n",
      "all\n",
      "Posted\n",
      "Credit\n",
      "Support\n",
      "held\n",
      "by\n",
      "the\n",
      "Obligee\n",
      "exceeds\n",
      "\n",
      "\n",
      "the\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\"\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      "\"\n",
      "means\n",
      ",\n",
      "unless\n",
      "otherwise\n",
      "specified\n",
      "in\n",
      "Paragraph\n",
      "13\n",
      ",\n",
      "for\n",
      "any\n",
      "Valuation\n",
      "Date\n",
      "\n",
      "\n",
      "(\n",
      "i\n",
      ")\n",
      "the\n",
      "Obligee\n",
      "'s\n",
      "Exposure\n",
      "for\n",
      "that\n",
      "Valuation\n",
      "Date\n",
      "plus\n",
      "(\n",
      "ii\n",
      ")\n",
      "the\n",
      "aggregate\n",
      "of\n",
      "all\n",
      "Independent\n",
      "Amounts\n",
      "applicable\n",
      "to\n",
      "the\n",
      "Obligor\n",
      ",\n",
      "if\n",
      "any\n",
      ",\n",
      "minus\n",
      "(\n",
      "iii\n",
      ")\n",
      "all\n",
      "Independent\n",
      "Amounts\n",
      "applicable\n",
      "to\n",
      "the\n",
      "Obligee\n",
      ",\n",
      "if\n",
      "any\n",
      ",\n",
      "minus\n",
      "(\n",
      "iv\n",
      ")\n",
      "the\n",
      "Obligor\n",
      "'s\n",
      "Threshold\n",
      ";\n",
      "provided\n",
      "however\n",
      ",\n",
      "that\n",
      "the\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      "will\n",
      "be\n",
      "deemed\n",
      "to\n",
      "be\n",
      "zero\n",
      "whenever\n",
      "the\n",
      "calculation\n",
      "of\n",
      "the\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      "yields\n",
      "a\n",
      "number\n",
      "less\n",
      "than\n",
      "zero\n"
     ]
    }
   ],
   "source": [
    "for token in docx:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Amount.\n",
      "Return Amount.\n",
      "Return Amount.\n",
      "4 and 5,\n",
      "and 5, upon\n",
      "5, upon a\n",
      ", upon a demand\n",
      "upon a demand made\n",
      "a demand made by\n",
      "demand made by the\n",
      "made by the Obligor\n",
      "by the Obligor on\n",
      "the Obligor on or\n",
      "Obligor on or promptly\n",
      "on or promptly following\n",
      "or promptly following a\n",
      "promptly following a Valuation\n",
      "following a Valuation Date\n",
      "a Valuation Date\n",
      "Valuation Date\n",
      "Date\n",
      "\n",
      "\n",
      "\n",
      "to the Obligor Posted\n",
      "the Obligor Posted Credit\n",
      "Obligor Posted Credit Support\n",
      "Posted Credit Support as\n",
      "Credit Support as specified\n",
      "Support as specified by\n",
      "as specified by the\n",
      "specified by the Obligor\n",
      "by the Obligor in\n",
      "the Obligor in that\n",
      "Obligor in that demand\n",
      "in that demand having\n",
      "that demand having a\n",
      "demand having a Value\n",
      "having a Value as\n",
      "a Value as of\n",
      "Value as of the\n",
      "as of the date\n",
      "of the date of\n",
      "the date of Transfer\n",
      "date of Transfer as\n",
      "of Transfer as close\n",
      "Transfer as close as\n",
      "as close as practicable\n",
      "close as practicable to\n",
      "as practicable to the\n",
      "practicable to the applicable\n",
      "to the applicable Return\n",
      "the applicable Return Amount\n",
      "applicable Return Amount (\n",
      "Return Amount (rounded\n",
      "Amount (rounded pursuant\n",
      "(rounded pursuant to\n",
      "rounded pursuant to Paragraph\n",
      "pursuant to Paragraph 13\n",
      "to Paragraph 13)\n",
      "Paragraph 13);\n",
      "13); provided\n",
      "); provided,\n",
      "; provided, however\n",
      "provided, however,\n",
      ", however, that\n",
      "however, that where\n",
      ", that where such\n",
      "that where such Posted\n",
      "where such Posted Credit\n",
      "such Posted Credit Support\n",
      "Posted Credit Support consists\n",
      "Credit Support consists of\n",
      "Support consists of Posted\n",
      "consists of Posted Lending\n",
      "of Posted Lending Collateral\n",
      "Posted Lending Collateral in\n",
      "Lending Collateral in the\n",
      "Collateral in the form\n",
      "in the form of\n",
      "the form of securities\n",
      "form of securities,\n",
      "of securities, the\n",
      "securities, the Obligee\n",
      ", the Obligee may\n",
      "the Obligee may Transfer\n",
      "Obligee may Transfer to\n",
      "may Transfer to the\n",
      "Transfer to the Obligor\n",
      "to the Obligor Equivalent\n",
      "the Obligor Equivalent Collateral\n",
      "Obligor Equivalent Collateral or\n",
      "Equivalent Collateral or repay\n",
      "Collateral or repay cash\n",
      "or repay cash equivalent\n",
      "repay cash equivalent thereof\n",
      "cash equivalent thereof in\n",
      "equivalent thereof in the\n",
      "thereof in the currency\n",
      "in the currency in\n",
      "the currency in which\n",
      "currency in which such\n",
      "in which such securities\n",
      "which such securities are\n",
      "such securities are denominated\n",
      "securities are denominated or\n",
      "are denominated or in\n",
      "denominated or in Termination\n",
      "or in Termination Currency\n",
      "in Termination Currency.\n",
      "Termination Currency.\n",
      "Currency.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in docx.sents:\n",
    "    for token in (sent):\n",
    "        if len(sent)<4:\n",
    "            print(sent)\n",
    "        else:\n",
    "            span=sent[token.i:token.i+4]\n",
    "            print(span.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i,sent in enumerate(docx.sents):\n",
    "    print(i,' ----> ',sent,'\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for sent in docx.sents:\n",
    "    print(sent)\n",
    "    \n",
    "    print(displacy.render(sent,options={'compact':True,'distance': 100,\n",
    "                                  'collapse_punct': True,'collapse_phrases':True}))\n",
    "    for ent in sent.ents:\n",
    "        print((ent.text,ent.label_))\n",
    "    print('------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(dir(sent))\n",
    "\n",
    "(sent.subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "pos=[token.pos_ for token in docx]\n",
    "counter=Counter(pos)\n",
    "\n",
    "kk=[[key for key in counter.keys()],[value for value in counter.values()]]\n",
    "\n",
    "pd.DataFrame(np.array(kk).T, columns=['Letter','Count'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b = {}\n",
    "for item in a:\n",
    "    b[item] = b.get(item, 0) + 1\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq = [[['a','b','a','c'],['NOUN', 'PP','VBG','VBD']], [['a','b','c','d','e','f'],['NOUN', 'PP','VBG','VBD','LL','GGWP']]]\n",
    "#print(len(seq))\n",
    "#for i in seq:\n",
    "   # if not len(set(i)) < 5:\n",
    "    #    print(set(i))\n",
    "     #   print(len(set(i)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_pos = []\n",
    "for i,sent in enumerate(docx.sents):\n",
    "    pos=[token.pos_ for token in sent]\n",
    "    token=[token.text for token in sent]\n",
    "    #print(i,' --> ',pos,'\\n')\n",
    "    list_pos.append([token, pos])\n",
    "for i,list in enumerate (list_pos):\n",
    "    print(i,'  ---->  ',list,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(wordlist, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        ngrams.append(wordlist[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(len(seq)):\n",
    "    print(getNGrams(seq[i][0],3))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_grammed=[]\n",
    "for i in range(len(list_pos)):\n",
    "    for j in range(len(list_pos[i]))\n",
    "        if not len(list_pos[i])<3:\n",
    "            n_grammed.append(getNGrams(list_pos[i],3))\n",
    "for i in range(len(n_grammed)):\n",
    "    for j in range\n",
    "    print(i+1, '-->' ,n_grammed[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counter = {}\n",
    "for jj in n_grammed[1]:\n",
    "    kk = tuple(jj)\n",
    "    if kk in counter.keys():\n",
    "        counter[kk] += 1\n",
    "    else:\n",
    "        counter[kk] = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import operator\n",
    "\n",
    "#counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import operator\n",
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_x = sorted(x.items(), key=lambda kv: kv[1])\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "doc1 =nlp('''Eligible Collateral or Posted Collateral that is:\n",
    "\n",
    "an amount of cash, the Base Currency Equivalent of the amount thereof multiplied by the applicable Valuation Percentage, if any;\n",
    "\n",
    "JGBs (as defined below) and other securities of which bid and offer quotations are generally available to the Valuation Agent in the over-the-counter market, the Base Currency Equivalent of the bid price quotation obtained by the Valuation Agent multiplied by the applicable Valuation Percentage, if any;\n",
    "\n",
    "a security that is primarily traded on a recognised securities exchange, the Base Currency Equivalent of the closing price on the exchange or, in the absence of such a closing price, the last bid quotation for the securities on the exchange multiplied by the applicable Valuation Percentage, if any; and\n",
    "\n",
    "Cash Deposit, the Base Currency Equivalent of the face amount thereof.''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from __future__ import print_function\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag,word_tokenize,tokenize\n",
    "def extract_phrases(my_tree, phrase):\n",
    "    my_phrases = []\n",
    "    if my_tree.label() == phrase:\n",
    "        my_phrases.append(my_tree.copy(True))\n",
    "\n",
    "    for child in my_tree:\n",
    "        if type(child) is nltk.Tree:\n",
    "            list_of_phrases = extract_phrases(child, phrase)\n",
    "            if len(list_of_phrases) > 0:\n",
    "                my_phrases.extend(list_of_phrases)\n",
    "\n",
    "    return my_phrases\n",
    "\n",
    "\n",
    "\n",
    "sentences = [\"The little yellow dog barked at the cat\",\n",
    "                 \"He studies Information Technology\"]\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>|<NNP>*}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "for x in sentences:\n",
    "    sentence = pos_tag(tokenize.word_tokenize(x))\n",
    "    tree = cp.parse(sentence)\n",
    "    print (\"\\n\",'Noun phrases:')\n",
    "    list_of_noun_phrases = extract_phrases(tree, 'NP')\n",
    "    for phrase in list_of_noun_phrases:\n",
    "        print (phrase, \"_\".join([x[0] for x in phrase.leaves()]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]'''\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp1 = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp1)\n",
    "\n",
    "doc = nlp1(u'Note: Since the fourteenth century the practice of “medicine” has become a profession; and more importantly, it\\'s a male-dominated profession.')\n",
    "[token.text for token in doc]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with docx.retokenize() as retokenizer:\n",
    "    for np in (docx.noun_chunks):\n",
    "        attrs = {\n",
    "            \"tag\": np.root.tag_,\n",
    "            \"lemma\": np.root.lemma_,\n",
    "            \"ent_type\": np.root.ent_type_,\n",
    "        }\n",
    "        retokenizer.merge(np, attrs=attrs)\n",
    "        \n",
    "spans = []\n",
    "\n",
    "for word in docx[:-1]:\n",
    "            if word.is_punct or not word.nbor(1).is_punct:\n",
    "                continue\n",
    "            start = word.i\n",
    "            end = word.i + 1\n",
    "            while end < len(docx) and docx[end].is_punct:\n",
    "                end += 1\n",
    "            span = docx[start:end]\n",
    "            spans.append((span, word.tag_, word.lemma_, word.ent_type_))\n",
    "with docx.retokenize() as retokenizer:        \n",
    "            for span, tag, lemma, ent_type in spans:\n",
    "                attrs = {\"tag\": tag, \"lemma\": lemma, \"ent_type\": ent_type}\n",
    "                retokenizer.merge(span, attrs=attrs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=nlp('''Return Amount. Subject to Paragraphs 4 and 5, upon a demand made by the Obligor on or promptly following a Valuation Date, if the Return Amount for that Valuation Date equals or exceeds the Obligee's Minimum Transfer Amount, then the Obligee will Transfer to the Obligor Posted Credit Support as specified by the Obligor in that demand having a Value as of the date of Transfer as close as practicable to the applicable Return Amount (rounded pursuant to Paragraph 13); provided, however, that where such Posted Credit Support consists of Posted Lending Collateral in the form of securities, the Obligee may Transfer to the Obligor Equivalent Collateral or repay cash equivalent thereof in the currency in which such securities are denominated or in Termination Currency. For this purpose, Transfer of the cash equivalent of any Posted Lending Collateral shall be deemed to be a return of such Posted Lending Collateral. However, solely for the purpose of this Paragraph 3(b), the Obligee's right to repay any cash equivalent is subject to the prior consent of the Obligor. Unless otherwise specified in Paragraph 13, the \"Return Amount\" applicable to the Obligee for any Valuation Date will equal the amount by which:\n",
    "\n",
    "the Value as of that Valuation Date of all Posted Credit Support held by the Obligee exceeds\n",
    "the Credit Support Amount.\n",
    "\n",
    "\"Credit Support Amount\" means, unless otherwise specified in Paragraph 13, for any Valuation Date\n",
    "(i) the Obligee's Exposure for that Valuation Date plus (ii) the aggregate of all Independent Amounts applicable to the Obligor, if any, minus (iii) all Independent Amounts applicable to the Obligee, if any, minus (iv) the Obligor's Threshold; provided however, that the Credit Support Amount will be deemed to be zero whenever the calculation of the Credit Support Amount yields a number less than zero''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "inpt=[(1,'abc'),(2,'def')]\n",
    "zipped=zip(*inpt)\n",
    "print(list(list(zipped)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "a=(5,3,'abc')\n",
    "b=(5,3,'abc')\n",
    "if a==b:\n",
    "    print ('Yes')\n",
    "else:\n",
    "    print('Rub it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made\n",
      "promptly following\n",
      "exceeds\n",
      "will Transfer\n",
      "specified\n",
      "having\n",
      "rounded\n",
      "provided\n",
      "consists\n",
      "may Transfer\n",
      "repay\n",
      "are denominated\n",
      "shall be deemed\n",
      "be\n",
      "repay\n",
      "is\n",
      "otherwise specified\n",
      "will equal\n",
      "held\n",
      "exceeds\n",
      "otherwise specified\n",
      "provided\n",
      "will be deemed\n",
      "be\n",
      "yields\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import textacy\n",
    "sentence = '''Return Amount. Subject to Paragraphs 4 and 5, upon a demand made by the Obligor on or promptly following a Valuation Date, if the Return Amount for that Valuation Date equals or exceeds the Obligee's Minimum Transfer Amount, then the Obligee will Transfer to the Obligor Posted Credit Support as specified by the Obligor in that demand having a Value as of the date of Transfer as close as practicable to the applicable Return Amount (rounded pursuant to Paragraph 13); provided, however, that where such Posted Credit Support consists of Posted Lending Collateral in the form of securities, the Obligee may Transfer to the Obligor Equivalent Collateral or repay cash equivalent thereof in the currency in which such securities are denominated or in Termination Currency. For this purpose, Transfer of the cash equivalent of any Posted Lending Collateral shall be deemed to be a return of such Posted Lending Collateral. However, solely for the purpose of this Paragraph 3(b), the Obligee's right to repay any cash equivalent is subject to the prior consent of the Obligor. Unless otherwise specified in Paragraph 13, the \"Return Amount\" applicable to the Obligee for any Valuation Date will equal the amount by which:\n",
    "\n",
    "the Value as of that Valuation Date of all Posted Credit Support held by the Obligee exceeds\n",
    "the Credit Support Amount.\n",
    "\n",
    "\"Credit Support Amount\" means, unless otherwise specified in Paragraph 13, for any Valuation Date\n",
    "(i) the Obligee's Exposure for that Valuation Date plus (ii) the aggregate of all Independent Amounts applicable to the Obligor, if any, minus (iii) all Independent Amounts applicable to the Obligee, if any, minus (iv) the Obligor's Threshold; provided however, that the Credit Support Amount will be deemed to be zero whenever the calculation of the Credit Support Amount yields a number less than zero.'''\n",
    "pattern = r'<VERB>?<ADV>*<VERB>+'\n",
    "doc1 = textacy.make_spacy_doc(sentence, lang='en_core_web_lg')\n",
    "lists = textacy.extract.pos_regex_matches(doc1, pattern)\n",
    "for list in lists:\n",
    "    print(list.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return\n",
      "Amount\n",
      ".\n",
      "Subject\n",
      "to\n",
      "Paragraphs\n",
      "4\n",
      "and\n",
      "5\n",
      ",\n",
      "upon\n",
      "a\n",
      "demand\n",
      "made\n",
      "by\n",
      "the\n",
      "Obligor\n",
      "on\n",
      "or\n",
      "promptly\n",
      "following\n",
      "a\n",
      "Valuation\n",
      "Date\n",
      ",\n",
      "if\n",
      "the\n",
      "Return\n",
      "Amount\n",
      "for\n",
      "that\n",
      "Valuation\n",
      "Date\n",
      "equals\n",
      "or\n",
      "exceeds\n",
      "the\n",
      "Obligee\n",
      "'s\n",
      "Minimum\n",
      "Transfer\n",
      "Amount\n",
      ",\n",
      "then\n",
      "the\n",
      "Obligee\n",
      "will\n",
      "Transfer\n",
      "to\n",
      "the\n",
      "Obligor\n",
      "Posted\n",
      "Credit\n",
      "Support\n",
      "as\n",
      "specified\n",
      "by\n",
      "the\n",
      "Obligor\n",
      "in\n",
      "that\n",
      "demand\n",
      "having\n",
      "a\n",
      "Value\n",
      "as\n",
      "of\n",
      "the\n",
      "date\n",
      "of\n",
      "Transfer\n",
      "as\n",
      "close\n",
      "as\n",
      "practicable\n",
      "to\n",
      "the\n",
      "applicable\n",
      "Return\n",
      "Amount\n",
      "(\n",
      "rounded\n",
      "pursuant\n",
      "to\n",
      "Paragraph\n",
      "13\n",
      ")\n",
      ";\n",
      "provided\n",
      ",\n",
      "however\n",
      ",\n",
      "that\n",
      "where\n",
      "such\n",
      "Posted\n",
      "Credit\n",
      "Support\n",
      "consists\n",
      "of\n",
      "Posted\n",
      "Lending\n",
      "Collateral\n",
      "in\n",
      "the\n",
      "form\n",
      "of\n",
      "securities\n",
      ",\n",
      "the\n",
      "Obligee\n",
      "may\n",
      "Transfer\n",
      "to\n",
      "the\n",
      "Obligor\n",
      "Equivalent\n",
      "Collateral\n",
      "or\n",
      "repay\n",
      "cash\n",
      "equivalent\n",
      "thereof\n",
      "in\n",
      "the\n",
      "currency\n",
      "in\n",
      "which\n",
      "such\n",
      "securities\n",
      "are\n",
      "denominated\n",
      "or\n",
      "in\n",
      "Termination\n",
      "Currency\n",
      ".\n",
      "For\n",
      "this\n",
      "purpose\n",
      ",\n",
      "Transfer\n",
      "of\n",
      "the\n",
      "cash\n",
      "equivalent\n",
      "of\n",
      "any\n",
      "Posted\n",
      "Lending\n",
      "Collateral\n",
      "shall\n",
      "be\n",
      "deemed\n",
      "to\n",
      "be\n",
      "a\n",
      "return\n",
      "of\n",
      "such\n",
      "Posted\n",
      "Lending\n",
      "Collateral\n",
      ".\n",
      "However\n",
      ",\n",
      "solely\n",
      "for\n",
      "the\n",
      "purpose\n",
      "of\n",
      "this\n",
      "Paragraph\n",
      "3(b\n",
      ")\n",
      ",\n",
      "the\n",
      "Obligee\n",
      "'s\n",
      "right\n",
      "to\n",
      "repay\n",
      "any\n",
      "cash\n",
      "equivalent\n",
      "is\n",
      "subject\n",
      "to\n",
      "the\n",
      "prior\n",
      "consent\n",
      "of\n",
      "the\n",
      "Obligor\n",
      ".\n",
      "Unless\n",
      "otherwise\n",
      "specified\n",
      "in\n",
      "Paragraph\n",
      "13\n",
      ",\n",
      "the\n",
      "\"\n",
      "Return\n",
      "Amount\n",
      "\"\n",
      "applicable\n",
      "to\n",
      "the\n",
      "Obligee\n",
      "for\n",
      "any\n",
      "Valuation\n",
      "Date\n",
      "will\n",
      "equal\n",
      "the\n",
      "amount\n",
      "by\n",
      "which\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "Value\n",
      "as\n",
      "of\n",
      "that\n",
      "Valuation\n",
      "Date\n",
      "of\n",
      "all\n",
      "Posted\n",
      "Credit\n",
      "Support\n",
      "held\n",
      "by\n",
      "the\n",
      "Obligee\n",
      "exceeds\n",
      "\n",
      "\n",
      "the\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\"\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      "\"\n",
      "means\n",
      ",\n",
      "unless\n",
      "otherwise\n",
      "specified\n",
      "in\n",
      "Paragraph\n",
      "13\n",
      ",\n",
      "for\n",
      "any\n",
      "Valuation\n",
      "Date\n",
      "\n",
      "\n",
      "(\n",
      "i\n",
      ")\n",
      "the\n",
      "Obligee\n",
      "'s\n",
      "Exposure\n",
      "for\n",
      "that\n",
      "Valuation\n",
      "Date\n",
      "plus\n",
      "(\n",
      "ii\n",
      ")\n",
      "the\n",
      "aggregate\n",
      "of\n",
      "all\n",
      "Independent\n",
      "Amounts\n",
      "applicable\n",
      "to\n",
      "the\n",
      "Obligor\n",
      ",\n",
      "if\n",
      "any\n",
      ",\n",
      "minus\n",
      "(\n",
      "iii\n",
      ")\n",
      "all\n",
      "Independent\n",
      "Amounts\n",
      "applicable\n",
      "to\n",
      "the\n",
      "Obligee\n",
      ",\n",
      "if\n",
      "any\n",
      ",\n",
      "minus\n",
      "(\n",
      "iv\n",
      ")\n",
      "the\n",
      "Obligor\n",
      "'s\n",
      "Threshold\n",
      ";\n",
      "provided\n",
      "however\n",
      ",\n",
      "that\n",
      "the\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      "will\n",
      "be\n",
      "deemed\n",
      "to\n",
      "be\n",
      "zero\n",
      "whenever\n",
      "the\n",
      "calculation\n",
      "of\n",
      "the\n",
      "Credit\n",
      "Support\n",
      "Amount\n",
      "yields\n",
      "a\n",
      "number\n",
      "less\n",
      "than\n",
      "zero\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with doc1.retokenize() as retokenizer:\n",
    "    for np in (doc1.noun_chunks):\n",
    "        attrs = {\n",
    "            \"tag\": np.root.tag_,\n",
    "            \"lemma\": np.root.lemma_,\n",
    "            \"ent_type\": np.root.ent_type_,\n",
    "        }\n",
    "        retokenizer.merge(np, attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return\n",
      "Amount\n",
      "Paragraphs\n",
      "a demand\n",
      "the Obligor\n",
      "a Valuation Date\n",
      "the Return Amount\n",
      "that Valuation Date\n",
      "the Obligee's Minimum Transfer Amount\n",
      "the Obligee\n",
      "the Obligor Posted Credit Support\n",
      "the Obligor\n",
      "that demand\n",
      "a Value\n",
      "the date\n",
      "Transfer\n",
      "the applicable Return Amount\n",
      "Paragraph\n",
      "such Posted Credit Support\n",
      "Posted Lending Collateral\n",
      "the form\n",
      "securities\n",
      "the Obligee\n",
      "the Obligor Equivalent Collateral\n",
      "the currency\n",
      "such securities\n",
      "Termination Currency\n",
      "this purpose\n",
      "Transfer\n",
      "the cash equivalent\n",
      "any Posted Lending Collateral\n",
      "a return\n",
      "such Posted Lending Collateral\n",
      "the purpose\n",
      "the Obligee's right\n",
      "any cash equivalent\n",
      "the prior consent\n",
      "the Obligor\n",
      "Paragraph\n",
      "the \"Return Amount\n",
      "the Obligee\n",
      "any Valuation Date\n",
      "the amount\n",
      "the Value\n",
      "that Valuation Date\n",
      "all Posted Credit Support\n",
      "the Obligee\n",
      "the Credit Support Amount\n",
      "\"Credit Support Amount\n",
      "Paragraph\n",
      "any Valuation Date\n",
      "the Obligee's Exposure\n",
      "that Valuation Date\n",
      "the aggregate\n",
      "all Independent Amounts\n",
      "the Obligor\n",
      "all Independent Amounts\n",
      "the Obligee\n",
      "the Obligor's Threshold\n",
      "the Credit Support Amount\n",
      "the calculation\n",
      "the Credit Support Amount\n"
     ]
    }
   ],
   "source": [
    "for list in (doc1.noun_chunks):\n",
    "    print(list.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "nlp_ = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "matcher = Matcher(nlp_.vocab)\n",
    "matcher.add(\"Adjective and noun\", [{POS: 'ADJ'}, {POS: 'NOUN'}])\n",
    "\n",
    "doc = nlp_(u'Ram is an intelligent boy')\n",
    "matches = matcher(doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram PROPN [ 3.8127925e+00 -1.9911897e+00 -1.7172649e+00 -3.1604443e+00\n",
      " -2.9358613e+00  2.3076332e-01  1.2803768e+00  9.1266561e-01\n",
      " -1.3160579e+00  1.9712985e+00 -2.1349218e+00 -2.9560833e+00\n",
      " -2.5688872e+00  3.0676723e-01 -5.5924594e-01 -3.6354566e+00\n",
      "  1.2693670e+00  3.6776721e+00  6.3125286e+00  8.5274041e-01\n",
      "  4.5962110e-02 -4.6800833e+00  1.0464257e-01  3.2737930e+00\n",
      "  3.3953249e-02  3.5759956e-03  2.3355672e+00  6.0999161e-01\n",
      "  3.3977642e+00  5.4178400e+00  4.7702961e+00  2.8811634e-01\n",
      " -1.8691466e+00  9.0427309e-01 -8.7546974e-02  2.4818349e+00\n",
      " -1.8042883e+00  4.3512058e+00  3.9590120e+00 -1.8750298e+00\n",
      " -9.8875952e-01  3.2067366e+00 -2.1972451e+00 -5.7121527e-01\n",
      " -1.1285863e+00 -2.0550094e+00 -4.0492029e+00  9.5899975e-01\n",
      "  1.3658807e-01 -8.6198306e-01 -4.5526505e+00  2.1860013e+00\n",
      " -4.1743927e+00  7.7158785e-01 -4.6637481e-01  5.6888700e-01\n",
      " -1.5013341e+00 -1.5119462e+00 -5.6333714e+00  7.4763620e-01\n",
      " -4.5574594e-01 -7.8039825e-01  2.9254892e+00  2.8905437e+00\n",
      " -1.8080335e+00  3.1005967e+00 -9.3134022e-01  8.1130660e-01\n",
      "  5.1735526e-01 -4.2670217e+00 -1.1710192e+00 -3.0426373e+00\n",
      "  2.5432513e+00  7.9617004e+00 -9.9517924e-01 -2.0451679e+00\n",
      " -2.8508940e+00 -2.9566360e+00  5.0283418e+00 -6.7464775e-01\n",
      " -2.1888804e+00  1.5104907e+00 -4.3432102e+00  5.8849972e-01\n",
      " -4.7118959e+00  3.9906392e+00  4.6261115e+00 -2.2920499e+00\n",
      " -2.2245746e+00 -1.8505694e+00  2.7090452e+00  1.2172644e+00\n",
      " -3.6110525e+00  4.8982358e+00  1.0234611e+00  3.6768847e+00]\n",
      "is VERB [-0.20550188  1.7078713   4.6281514   1.9969561   3.71529    -0.74377924\n",
      "  3.3199239   0.4136207   1.9596981  -3.8040214  -2.102918   -1.0801731\n",
      " -3.8552632  -0.08916801 -1.267878   -5.4298816  -4.5429325   3.6033044\n",
      " -1.086228   -5.1989384  -3.6956053   4.5979714   2.0546439   0.86292243\n",
      "  1.9442953  -2.791292    2.2261162  -3.0703733   1.1736517  -2.206236\n",
      "  5.201376    0.17636967  0.50301653  2.593813    2.2202744   3.8361626\n",
      " -1.3647467   2.4323726  -3.8198526   3.1105099  -0.47983512  1.982235\n",
      "  0.9320434   0.5092112   3.497195   -0.819691    0.86172223  4.372491\n",
      " -1.3432717   2.6912286   0.02619928  0.6357176  -3.3406193   1.1919909\n",
      "  6.0741043  -0.9875859   0.126163    3.5419564  -0.21605644  0.7538511\n",
      " -3.6026576   3.7075624  -1.4114436   0.6143693  -1.0216155   4.8537326\n",
      " -2.6319566  -0.2922293  -2.71523    -5.537263    4.564952   -1.6518204\n",
      " -0.6403414   3.384615   -1.505235   -0.8228635  -3.2463732  -4.107255\n",
      " -1.4834613   0.16768456  2.2751231  -3.7805073   3.1928432  -2.1675658\n",
      "  1.6097815  -2.0410929  -2.8386045  -2.6844578  -0.19059157 -1.466758\n",
      "  1.7754987  -0.24895573 -1.7008998   5.087217    0.04397549 -5.190494  ]\n",
      "an DET [-4.8607187  -2.0722103  -4.9415126   1.6234347   0.29602545  4.6120625\n",
      " -2.5699582   3.6652832   1.7062392  -1.9766868   2.0950003   2.6137133\n",
      " -4.1155586   0.54624355 -1.9852878   0.2433458  -0.5189727   2.9077778\n",
      "  5.056203   -0.11442143  1.7890373  -1.8819962   2.4242377   0.17748839\n",
      " -1.1933017   1.8654593   2.020273   -3.249023    0.12202537  3.433899\n",
      "  3.1152425   1.5520874  -2.2507837  -3.3990703   0.3364337  -4.072514\n",
      "  4.1849823   2.16845    -4.892921    4.2961583   1.9439608   0.14155889\n",
      " -2.1073828   5.151807   -1.01604     0.04840505  0.99644995 -4.9193134\n",
      "  3.5355287  -5.4823422   3.172737   -2.9642906   0.07998693  0.799006\n",
      "  1.6034235  -0.569224   -0.410714   -0.541809   -4.1075487  -1.5877833\n",
      " -2.275663   -1.5082382   1.5773015   7.300066   -0.07331835  1.7245235\n",
      "  0.7931713  -4.1139917  -0.782925   -2.028385    2.9199429  -3.7117019\n",
      "  5.0258007  -0.09874482 -0.75727725  2.7146463  -0.9842866  -1.3522424\n",
      "  0.32446322 -2.200155   -4.390998    0.8410717   3.3592014  -4.0029197\n",
      "  2.864218    0.34444976  1.6358964  -4.4383087  -5.5936213  -0.51705754\n",
      "  5.220009    0.8809494  -3.5145478   4.172259    2.9048593  -3.7324507 ]\n",
      "intelligent ADJ [-1.5069599  -2.5613894  -0.5110507  -4.0827103   1.1517495   3.2127674\n",
      "  5.528097    1.6722077   0.41574472  3.3886871   2.765584    3.853967\n",
      " -3.847835   -2.3127787   0.6116732   4.586647   -0.9401883  -4.0410547\n",
      " -2.0751324  -1.0073271  -3.214684   -4.1639304   0.16343574  3.851575\n",
      "  2.1545935   3.2365413   1.0250595  -3.9745083   2.6617296   5.0937676\n",
      " -2.4745357  -0.17219943 -1.9395188  -2.4568462   2.2429807  -1.7292643\n",
      " -0.6576616  -2.4684324  -4.6972146   3.1325586   1.5959034   2.6718833\n",
      "  2.412888   -0.44054228 -3.7960339   0.20836419  1.1415656  -3.1563637\n",
      " -2.0575533  -0.60742515 -1.8220948  -1.4042705  -0.90076333 -3.2883584\n",
      " -0.9763824   2.3772078   3.3741803   5.357307   -3.206994   -1.2184513\n",
      " -2.4528918  -1.651612   -3.443437   -0.7345996   0.37526083 -1.4103693\n",
      "  0.02053735 -2.2925544  -4.7173023  -1.2490878  -0.14064614  3.3006485\n",
      "  0.5100492   5.13946     2.6163495   2.3140733  -0.8303896   1.7553623\n",
      "  2.1812983  -0.07412103 -1.5418122  -3.2846057  -2.2358222   6.982098\n",
      "  3.7781005   2.336727    5.6544385   1.6905358   0.260426   -3.198388\n",
      "  0.58928436  2.2195466  -1.2253878  -1.7159253  -4.2253075   0.30072674]\n",
      "boy NOUN [ 4.01686    -2.6807926  -1.0777365  -1.5786635  -3.4129295   0.72812986\n",
      "  0.649709    1.0186702  -3.0092046   3.4479172   1.1114142   1.3607562\n",
      " -4.8092785  -2.2659822   1.8899171  -0.16616294  2.3282504   4.2172065\n",
      "  1.3731463   2.7963347  -1.0080945   0.6642391   3.4382796   1.9048204\n",
      " -2.885816    0.89429986 -0.50869673 -0.8978506   2.428299    5.0646048\n",
      " -1.0378246  -2.5774975  -5.7804284  -0.7256789   3.670039    5.8522396\n",
      "  0.36791813  0.75036013 -1.4467149  -1.7657938  -3.0606284   2.5290186\n",
      " -0.50202847  5.5792694   1.3983068   1.9144186  -0.57679176  0.56891143\n",
      " -3.9249508  -1.4975224  -2.3951292  -2.483406   -2.4833355  -2.4751112\n",
      " -2.4848883  -1.0957521   4.5029345  -1.0133173  -1.2441556   2.7067869\n",
      " -0.12827808  0.5412999   7.5295706  -1.6219835  -1.8526195   0.69858533\n",
      "  1.405203    3.63715    -3.106969    6.263899    0.6965     -2.2501023\n",
      "  6.9662538   0.721536   -0.88196653  2.904458   -3.0837317  -0.26486695\n",
      " -3.5232348  -2.6622066  -2.0358236   0.6676394  -5.583335    1.1767286\n",
      "  1.4334679  -4.8769703  -0.63204896 -3.8687828   2.966109    0.3552186\n",
      " -3.1531172  -1.5909953   1.9604369  -2.636658   -2.6888678   4.754347  ]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss1='The President greets the press in chicago'\n",
    "ss2='Obama speaks to the media in Illinois'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect=TfidfVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [(i, j)\n",
    "           for i in range(10)\n",
    "           for j in range(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9]\n",
    "sqr=map(lambda x: x*x,a)\n",
    "s=list(sqr)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 2]]\n",
      "[[1 2 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([1,3,2])\n",
    "c=np.array([1,2,4])\n",
    "b=a.reshape(1,3)\n",
    "d=c.reshape(1,3)\n",
    "print(b)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1. 3. 2.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d61e3e643f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/exp/venv/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/exp/venv/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 111\u001b[0;31m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[1;32m    112\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[1;32m    113\u001b[0m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
      "\u001b[0;32m~/PycharmProjects/exp/venv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 3. 2.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim=cosine_similarity(a,c)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
